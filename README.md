# Вопросно-ответный LLM-сервис для поддержки пользователей платформы SMILE

![Архитектура сервиса](https://github.com/user-attachments/assets/48cd2d22-28d2-4166-bbb0-456e819804d7)

## Описание проекта

Данный проект реализует **готовый вопросно-ответный сервис** для поддержки пользователей платформы SMILE, построенный на базе современных языковых моделей (LLM), FastAPI и Angular.
Система предназначена для автоматизации технической поддержки и консультирования по эксплуатации платформы, обеспечивая поиск и выдачу релевантных ответов на пользовательские вопросы. Архитектура предполагает интеграцию гибридного поиска по базе знаний с fallback на генерацию ответа через LLM, а также использование полнофункционального веб-интерфейса и инфраструктуры контейнеризации.

### Основные компоненты:

1. **Бэкенд (FastAPI):**
   Отвечает за обработку запросов пользователей, взаимодействие с базой данных и работу с LLM (через внешний или локальный API).
2. **Фронтенд (Angular 19 + Vite):**
   Современный веб-интерфейс для ввода вопросов, просмотра истории и взаимодействия с системой.
3. **Nginx:**
   Используется для проксирования API и отдачи статики фронтенда.
4. **База данных (PostgreSQL):**
   Хранение истории взаимодействий, а также структуры базы знаний для поиска ответов.
5. **Docker и docker-compose:**
   Позволяют развернуть сервис в виде изолированных контейнеров.

---

## Структура проекта

```
assistant_service/
├── backend/                    # Серверная часть (FastAPI)
│   ├── Dockerfile.backend
│   ├── main.py
│   ├── requirements.txt
│   ├── .env
├── frontend/                   # Клиентская часть (Angular + Vite)
│   ├── Dockerfile.frontend
│   ├── src/
│   ├── package.json
│   ├── vite.config.ts
│   ├── .env
├── nginx/                      # Конфигурация nginx
│   ├── nginx.conf
├── docker-compose.yml
├── docker-compose.override.yml
└── README.md
```

---

## Установка и запуск

### 1. Локальная разработка (dev-режим)

```sh
git clone https://github.com/dinarasaurae/assistant_service.git
cd assistant_service

# Скопировать примеры конфигов
cp backend/.env.example backend/.env
cp frontend/.env.example frontend/.env

# Запуск контейнеров для разработки
docker-compose -f docker-compose.override.yml up --build
```

* Фронтенд будет доступен по адресу: [http://localhost:4200](http://localhost:4200)
* Бэкенд — на сервере (адрес и порт см. в `.env` и docker-compose).

### 2. Продакшн-развертывание

```sh
cp backend/.env.example backend/.env
cp frontend/.env.example frontend/.env
docker-compose up --build -d
```

* Основной веб-интерфейс: [http://94.126.205.209/](http://94.126.205.209/)

---

## API-документация (FastAPI)

### Основная конечная точка

#### `POST /ask`

* **Тело запроса:**

  ```json
  { "question": "Как авторизоваться в системе через Яндекс?" }
  ```

* **Пример ответа:**

  ```json
  {
    "search_query": "Как авторизоваться в системе через Яндекс?",
    "matched_question": "Какие действия необходимо выполнить для авторизации через социальную сеть Яндекс?",
    "answer": "Для авторизации через Яндекс, нажмите на соответствующую кнопку..."
  }
  ```

* Полная OpenAPI-документация:
  [http://94.126.205.209:8001/docs](http://94.126.205.209:8001/docs)

---

## Переменные окружения

**backend/.env:**

```
DB_USER=user
DB_PASSWORD=password
DB_NAME=chat_db
DB_HOST=db
DB_PORT=5432
LLM_API_URL=http://XXXXXXXXXX
```

**frontend/.env:**

```
VITE_API_URL=http://94.126.205.209:8001
```

---

## Кратко о внутренней архитектуре

Архитектура сервиса реализует промышленный пайплайн поиска ответов с поддержкой:

* лексического и семантического поиска по базе знаний,
* генерации релевантных синонимов и уточнений запросов,
* fallback к генеративной LLM для новых или нераспознанных вопросов,
* хранения истории сессий и аудита взаимодействий.

Алгоритмы поиска и фильтрации разработаны и оптимизированы на базе проведённого прикладного исследования (см. [research-metrics-репозиторий](https://github.com/dinarasaurae/assistant_service_research) и см. [research-репозиторий](https://github.com/dinarasaurae/ai_assistant_investigations)), в котором обоснованы выбранные модели и параметры, а также достигнуты метрики: F1-score ≈ 0.82, recall ≈ 0.7, precision ≈ 0.94.

---

## Планы развития

* Доработка и дообучение векторных моделей на пользовательских данных
* Оптимизация скорости отклика при генерации LLM-ответа
* Интеграция с дополнительными источниками базы знаний
* Улучшение управления сессиями пользователей и расширение аудита

---

## публикация

Подробные результаты экспериментов, описание гибридной архитектуры поиска, сравнение метрик и подбор параметров доступны в [репозитории исследований](https://github.com/dinarasaurae/qa-assistant-method-comparison) и в научной статье: [статья](Хисаметдинова_Д.Н._статья.pdf)

